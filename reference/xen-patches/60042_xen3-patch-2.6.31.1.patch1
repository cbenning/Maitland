Subject: Linux 2.6.31.1
From: Greg Kroah-Hartman <gregkh@suse.de>

Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

Automatically created from "patches.kernel.org/patch-2.6.31.1" by xen-port-patches.py

--- head-2009-10-01.orig/arch/x86/include/mach-xen/asm/processor.h	2009-09-02 11:59:54.000000000 +0200
+++ head-2009-10-01/arch/x86/include/mach-xen/asm/processor.h	2009-10-01 11:35:45.000000000 +0200
@@ -411,7 +411,17 @@ extern unsigned long kernel_eflags;
 extern asmlinkage void ignore_sysret(void);
 #else	/* X86_64 */
 #ifdef CONFIG_CC_STACKPROTECTOR
-DECLARE_PER_CPU(unsigned long, stack_canary);
+/*
+ * Make sure stack canary segment base is cached-aligned:
+ *   "For Intel Atom processors, avoid non zero segment base address
+ *    that is not aligned to cache line boundary at all cost."
+ * (Optim Ref Manual Assembly/Compiler Coding Rule 15.)
+ */
+struct stack_canary {
+	char __pad[20];		/* canary at %gs:20 */
+	unsigned long canary;
+};
+DECLARE_PER_CPU(struct stack_canary, stack_canary) ____cacheline_aligned;
 #endif
 #endif	/* X86_64 */
 
--- head-2009-10-01.orig/arch/x86/include/mach-xen/asm/system.h	2009-09-09 11:31:51.000000000 +0200
+++ head-2009-10-01/arch/x86/include/mach-xen/asm/system.h	2009-10-01 11:35:45.000000000 +0200
@@ -30,7 +30,7 @@ void __switch_to_xtra(struct task_struct
 	"movl %P[task_canary](%[next]), %%ebx\n\t"			\
 	"movl %%ebx, "__percpu_arg([stack_canary])"\n\t"
 #define __switch_canary_oparam						\
-	, [stack_canary] "=m" (per_cpu_var(stack_canary))
+	, [stack_canary] "=m" (per_cpu_var(stack_canary.canary))
 #define __switch_canary_iparam						\
 	, [task_canary] "i" (offsetof(struct task_struct, stack_canary))
 #else	/* CC_STACKPROTECTOR */
--- head-2009-10-01.orig/arch/x86/kernel/cpu/common-xen.c	2009-08-17 14:46:34.000000000 +0200
+++ head-2009-10-01/arch/x86/kernel/cpu/common-xen.c	2009-10-01 11:35:45.000000000 +0200
@@ -1098,7 +1098,7 @@ DEFINE_PER_CPU(struct orig_ist, orig_ist
 #else	/* CONFIG_X86_64 */
 
 #ifdef CONFIG_CC_STACKPROTECTOR
-DEFINE_PER_CPU(unsigned long, stack_canary);
+DEFINE_PER_CPU(struct stack_canary, stack_canary) ____cacheline_aligned;
 #endif
 
 /* Make sure %fs and %gs are initialized properly in idle threads */
--- head-2009-10-01.orig/arch/x86/kernel/head_32-xen.S	2009-09-02 11:59:54.000000000 +0200
+++ head-2009-10-01/arch/x86/kernel/head_32-xen.S	2009-10-01 11:35:45.000000000 +0200
@@ -69,7 +69,6 @@ ENTRY(startup_32)
 	 */
 	movl $per_cpu__gdt_page,%eax
 	movl $per_cpu__stack_canary,%ecx
-	subl $20, %ecx
 	movw %cx, 8 * GDT_ENTRY_STACK_CANARY + 2(%eax)
 	shrl $16, %ecx
 	movb %cl, 8 * GDT_ENTRY_STACK_CANARY + 4(%eax)
--- head-2009-10-01.orig/arch/x86/mm/pageattr-xen.c	2009-08-17 14:46:34.000000000 +0200
+++ head-2009-10-01/arch/x86/mm/pageattr-xen.c	2009-10-01 11:35:45.000000000 +0200
@@ -843,6 +843,7 @@ static int change_page_attr_set_clr(unsi
 {
 	struct cpa_data cpa;
 	int ret, cache, checkalias;
+	unsigned long baddr = 0;
 
 	/*
 	 * Check, if we are requested to change a not supported
@@ -874,6 +875,11 @@ static int change_page_attr_set_clr(unsi
 			 */
 			WARN_ON_ONCE(1);
 		}
+		/*
+		 * Save address for cache flush. *addr is modified in the call
+		 * to __change_page_attr_set_clr() below.
+		 */
+		baddr = *addr;
 	}
 
 	/* Must avoid aliasing mappings in the highmem code */
@@ -921,7 +927,7 @@ static int change_page_attr_set_clr(unsi
 			cpa_flush_array(addr, numpages, cache,
 					cpa.flags, pages);
 		} else
-			cpa_flush_range(*addr, numpages, cache);
+			cpa_flush_range(baddr, numpages, cache);
 	} else
 		cpa_flush_all(cache);
 
